{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXPH0w4-_a9P"
   },
   "source": [
    "# Sprint25課題 Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwDgJVUe_a6-"
   },
   "source": [
    "## 1.この課題の目的\n",
    "\n",
    "- RNNの活用例を知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoavuHh5_a4q"
   },
   "source": [
    "## 2.機械翻訳\n",
    "\n",
    "RNNの最も基本的な活用例としては機械翻訳がある。\n",
    "\n",
    "これは時系列データを入力し、時系列を出力するSequence to Sequenceの手法によって実現できる。\n",
    "\n",
    "### 【問題1】機械翻訳の実行とコードリーディング\n",
    "Keras公式のサンプルコードを実行し、短い英語からフランス語への変換を行う。\n",
    "\n",
    "[keras/lstm_seq2seq.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "\n",
    "その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめること。\n",
    "\n",
    "以下のように、どこからどこの行が何をしているかを記述する。\n",
    "\n",
    "（例）\n",
    "\n",
    "- 51から55行目 : ライブラリのimport\n",
    "- 57から62行目 : ハイパーパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Ym7TptdnCkJo",
    "outputId": "19893b6f-d421-4b57-ce37-996b2c22c74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab  import drive\n",
    "drive.mount(\"gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlRdMrUDAw36"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, CuDNNLSTM\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGyQRZy8CIAD"
   },
   "outputs": [],
   "source": [
    "folder_path =  '/content/gdrive/My Drive/My Documents/Study/Programming/DIVE INTO CODE/Class/sprint25'\n",
    "im2seq_path = '/content/gdrive/My Drive/My Documents/Study/Programming/DIVE INTO CODE/Class/sprint25/image_captioning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3lAwt6jCxXY"
   },
   "outputs": [],
   "source": [
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojWHTc6TBM2_"
   },
   "source": [
    "#### Data Source\n",
    "- [Tab-delimited Bilingual Sentence Pairs ](http://www.manythings.org/anki/)\n",
    "- コードの役割は、コード中にコメントとして記載した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1EOS-5f-AcwP",
    "outputId": "575e1eb1-9656-496f-99d9-83449b7e189b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0628 06:27:18.475404 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0628 06:27:18.507002 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0628 06:27:21.029186 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0628 06:27:21.510671 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0628 06:27:21.536674 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0628 06:27:21.652881 140470350116736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0628 06:27:22.125519 140470350116736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.9140 - val_loss: 0.9270\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.7269 - val_loss: 0.7571\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.6180 - val_loss: 0.6755\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.5621 - val_loss: 0.6249\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.5232 - val_loss: 0.5930\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4922 - val_loss: 0.5702\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4674 - val_loss: 0.5400\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.4457 - val_loss: 0.5224\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4264 - val_loss: 0.5120\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4083 - val_loss: 0.4995\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3925 - val_loss: 0.4869\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3778 - val_loss: 0.4813\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3640 - val_loss: 0.4680\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3512 - val_loss: 0.4729\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3386 - val_loss: 0.4591\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3273 - val_loss: 0.4541\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3165 - val_loss: 0.4506\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3056 - val_loss: 0.4542\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.2959 - val_loss: 0.4479\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.2865 - val_loss: 0.4541\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.2773 - val_loss: 0.4470\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.2687 - val_loss: 0.4475\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.2607 - val_loss: 0.4499\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.2525 - val_loss: 0.4470\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.2449 - val_loss: 0.4492\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.2378 - val_loss: 0.4505\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.2307 - val_loss: 0.4511\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.2240 - val_loss: 0.4514\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.2177 - val_loss: 0.4582\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.2113 - val_loss: 0.4648\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.2056 - val_loss: 0.4624\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1999 - val_loss: 0.4674\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1945 - val_loss: 0.4678\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1889 - val_loss: 0.4764\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1842 - val_loss: 0.4744\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.1792 - val_loss: 0.4765\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1746 - val_loss: 0.4766\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1695 - val_loss: 0.4872\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.1657 - val_loss: 0.4899\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1612 - val_loss: 0.4970\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1571 - val_loss: 0.4989\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1533 - val_loss: 0.5079\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1498 - val_loss: 0.5064\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.1463 - val_loss: 0.5099\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1424 - val_loss: 0.5170\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1396 - val_loss: 0.5214\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.1363 - val_loss: 0.5262\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1333 - val_loss: 0.5299\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1303 - val_loss: 0.5357\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1276 - val_loss: 0.5332\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1244 - val_loss: 0.5399\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.1217 - val_loss: 0.5515\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1194 - val_loss: 0.5495\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1166 - val_loss: 0.5588\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1143 - val_loss: 0.5583\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1121 - val_loss: 0.5640\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.1095 - val_loss: 0.5699\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.1078 - val_loss: 0.5746\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.1055 - val_loss: 0.5786\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.1033 - val_loss: 0.5799\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.1012 - val_loss: 0.5827\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0999 - val_loss: 0.5925\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0976 - val_loss: 0.5937\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.0958 - val_loss: 0.6022\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.0939 - val_loss: 0.6062\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0924 - val_loss: 0.6069\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0911 - val_loss: 0.6139\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.0890 - val_loss: 0.6117\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0878 - val_loss: 0.6197\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0859 - val_loss: 0.6212\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.0843 - val_loss: 0.6240\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0831 - val_loss: 0.6329\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.0816 - val_loss: 0.6290\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.0802 - val_loss: 0.6383\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0791 - val_loss: 0.6439\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0774 - val_loss: 0.6418\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0765 - val_loss: 0.6468\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.0753 - val_loss: 0.6477\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0743 - val_loss: 0.6510\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0729 - val_loss: 0.6589\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.0714 - val_loss: 0.6598\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0704 - val_loss: 0.6583\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0692 - val_loss: 0.6660\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0682 - val_loss: 0.6741\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.0671 - val_loss: 0.6752\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0664 - val_loss: 0.6840\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0654 - val_loss: 0.6835\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0644 - val_loss: 0.6919\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0634 - val_loss: 0.6867\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0625 - val_loss: 0.6908\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0615 - val_loss: 0.6986\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0609 - val_loss: 0.7097\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0601 - val_loss: 0.7061\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0590 - val_loss: 0.7058\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.0583 - val_loss: 0.7130\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0576 - val_loss: 0.7151\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0568 - val_loss: 0.7151\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0561 - val_loss: 0.7250\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.0551 - val_loss: 0.7228\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.0545 - val_loss: 0.7201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_1/strided_slice_16:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'cu_dnnlstm_1/strided_slice_17:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuivez.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Bavou !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Bavou !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée amoureuse.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée amoureuse.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais le français.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J’ai payé.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: J'en suis sûre.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: En aucune manière !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous achetons.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous gagnâmes.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez gentille !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entrez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "latent_dim = 256 # 符号化空間の潜在次元\n",
    "num_samples = 10000 # 使用するテキスト数の上限\n",
    "data_path = os.path.join(folder_path, 'fra-eng/fra.txt') # データ格納先\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = [] # 英語\n",
    "target_texts = [] # 仏語\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    # 英仏テキストの組み合わせを改行文字（\"\\n\"）で分割\n",
    "    lines = f.read().split('\\n')\n",
    "# 最大num_samples個のテキストの組み合わせを、入力値とターゲットに分割する\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # 水平タブ（\"\\t\"）をスタート、改行（\"\\n\"）をターゲット文章の終わりとする\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    # 入力、およびターゲットの単語リストを作成\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# 生成した単語リストをアルファベット順にソートする\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# 語彙数\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# 最も長いテキストの単語数をカウントする\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# 単語をIDに変換する辞書\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Enncoderのデータ（入力値の単語ベクトル）を保存する配列 shape(samples, sequence, vocab)を初期化\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# Decoderのデータ（ターゲットの単語ベクトル）を保存する配列 shape(samples, sequence, vocab)を初期化\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# Decoderのデータ（ターゲットの単語ベクトル）を1シーケンスずらして保存する配列 shape(samples, sequence, vocab) => 推論に用いる\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 単語ベクトルの生成\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # 入力値の単語ベクトルを生成\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # ターゲットの単語ベクトルを生成\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # ターゲットの単語ベクトルを1シーケンスずらして保存 => 推論に用いる\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# モデルの構築（Encoder）\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# モデルの構築（Decoder）\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# モデルの定義\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# コンパイル、学習\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# モデルを保存\n",
    "model.save('s2s.h5')\n",
    "\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states) # encoder_inputs = Input(shape=(None, num_encoder_tokens)) & encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# IDを単語に変換する辞書\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 入力値をEncodingし、隠れ状態を予測する\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Decodingしたテキストを格納する配列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 出力値の最初の文字は\"水平タブ\"とする\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    \n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 入力したテキスト（英語）をEncode/Decodeした後のテキスト（仏語）を出力する\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KKhzq2hfv-QJ"
   },
   "source": [
    "- 英仏翻訳が機能していることを確認した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AhVjer7_a2K"
   },
   "source": [
    "## 3.イメージキャプショニング\n",
    "\n",
    "- 他の活用例としてイメージキャプショニングがある。（画像に対する説明の文章を推定するタスク）\n",
    "\n",
    "- これは画像を入力し、時系列を出力するImage to Sequenceの手法によって行われる。\n",
    "\n",
    " [pytorch-tutorial/tutorials/03-advanced/image_captioning at master · yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n",
    "\n",
    "- イメージキャプショニングは学習に多くの時間がかかるため、ここでは学習済みの重みが公開されている実装を動かす。\n",
    "\n",
    "- Kerasには平易に扱える実装が公開されていないため、今回はPyTorchによる実装を扱う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qT8A7GDQ_az1"
   },
   "source": [
    "### 【問題2】イメージキャプショニングの学習済みモデルの実行\n",
    "\n",
    "- 上記実装において、**5. Test the model**の項目を実行し、また、自身で用意した画像に対しても文章を生成する。\n",
    "\n",
    "- これらに対してどういった文章が出力されたかを記録して提出すること。\n",
    "\n",
    "- データセットからの学習は行わず、学習済みの重みをダウンロードして利用する。\n",
    "\n",
    "- 注意点：デフォルトで設定されている重みのファイル名とダウンロードできる重みのファイル名は異なっているため、書き換える必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEYrkj_D_aut"
   },
   "source": [
    "### Image Captioning\n",
    "\n",
    "- Image Captioningの目的：与えられた入力画像を自然言語の記述に変換する\n",
    "- Image Encoder：畳み込みニューラルネットワーク（Convolutional Neural Network）\n",
    "- Model：ILSVRC-2012-CLS画像分類データセットで事前学習されたresnet-152モデルを使用 \n",
    "- Decoder：Long short term memory Network\n",
    "\n",
    " ![Model Captioning](https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/image_captioning/png/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qoye1DfPscX4",
    "outputId": "caf9932a-ffb9-42b0-f2c2-e7e57db53aaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/gdrive/My Drive/My Documents/Study/Programming/DIVE INTO CODE/Class/sprint25/image_captioning'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(im2seq_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOZnAm7Syo2y"
   },
   "source": [
    "### サンプルコードを動かす\n",
    "- [画像元] (https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/png/example.png)\n",
    "- ![キリン](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/png/example.png?raw=true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AtJLUM_KsT6-",
    "outputId": "4bb808ee-7681-415c-fae9-84b8d46cebdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a group of giraffes standing next to each other . <end>\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --image=\"./image/example.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQ5f6peFvjpX"
   },
   "source": [
    "### 用意した画像を自然言語に変換する\n",
    "\n",
    "- [参照] [PAKUTASO（フリー素材）](https://www.pakutaso.com/20160230047post-6972.html)\n",
    "\n",
    " <画像１：ウユニ塩湖>\n",
    " ![代替テキスト](https://www.pakutaso.com/shared/img/thumb/MIYA160131480493_TP_V.jpg)\n",
    "\n",
    " <画像２：水鏡に映る中尊寺弁財天堂>\n",
    " ![代替テキスト](https://www.pakutaso.com/shared/img/thumb/AME20181113A004_TP_V.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONHieFvDwIVE"
   },
   "source": [
    "#### 生成した文章は以下のとおり。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "OijrbfCnvGnj",
    "outputId": "3aa95e96-6301-4e6b-8103-3811da235f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uyuni\n",
      "<start> a view of a lake with a boat in the background . <end>\n",
      "\n",
      "Temple\n",
      "<start> a close up of a bench in the woods <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Uyuni\")\n",
    "!python sample.py --image=\"./image/uyuni1.jpg\"\n",
    "\n",
    "print(\"\\nTemple\")\n",
    "!python sample.py --image=\"./image/temple1.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXIzM50m_2OO"
   },
   "source": [
    "### 【問題3】Kerasで動かしたい場合はどうするかを調査\n",
    "\n",
    "PyTorchによる実装を動かしたが、何らかの理由からKerasで動かしたい状況が考えられる。\n",
    "\n",
    "- どういった手順を踏むことになるか調査し、できるだけ詳しく説明すること。\n",
    "\n",
    "- 特に、今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れること。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qte-jc02N6s"
   },
   "source": [
    "#### PytorchによるモデルをKerasで動かす方法\n",
    "\n",
    "1. PytorchによるモデルのアーキテクチャをKerasで再構築する。（レイヤーは１対１対応する）\n",
    "  - 入力、出力の形状を揃える\n",
    "  - カーネルサイズを揃える\n",
    "  - ストライド、パディングの方法を揃える\n",
    "\n",
    "\n",
    "2. Kerasで再構築したモデルの各レイヤーに対して、Pytorchのレイヤーの重みを共有する。\n",
    "  - Pytorchのモデルに対して**.state_dict()メソッド**を適用し、各レイヤーの重みを抽出する。\n",
    "      - アウトプットのデータ型は\"pythonの辞書型\"であり、keyはレイヤーの名称、valueは重みである。\n",
    "  \n",
    "  - Kerasのlayer.set_weights(weights)メソッドを用いて、Pytorchのモデルから抽出した重みを代入していく。注意点は以下の通り。\n",
    "    - 重みを格納した辞書のkeyであるレイヤーの名称を、KerasとPytorchで揃えること。\n",
    "    - Pytorchモデルで取得した重みのデータ型を、Pytorchの.numpy()メソッドでNumpy配列に変換すること。 （Pytorchから取得した重みの型はTensorに対して、set_weights()メソッドでkerasモデルに代入する重みの型はNumpyである必要がある）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqhkVf4lwOIo"
   },
   "source": [
    "### 参考にした情報\n",
    "\n",
    "- [Keras official document](https://keras.io/ja/layers/about-keras-layers/)\n",
    "- [Pytorch official document](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A03PtSWT0egS"
   },
   "source": [
    "#### Take away\n",
    "\n",
    "1. 機械翻訳やImage Captioningの事例を通じて、自然言語処理におけるrnnの有用性（文脈理解）を理解した。\n",
    "\n",
    "\n",
    "2. Image Captioningのように、複数領域（例えば、画像処理と自然言語処理を組み合わせる）にまたがるモデルを構築する際には、膨大なデータセット、学習時間を要するため、基本的には事前学習した重みを用いることが肝要（転移学習）。必要に応じてファインチューニングすると良い。\n",
    "\n",
    "\n",
    "3.  異なるフレームワーク間でモデルの重みを共有する方法に関しても理解が深まった。論文実装で公開されているコードは、Kerasでしか構築されていない、あるいはPytorchでしか構築されていないこともある。実際の課題や企業の課題に応じて利用するフレームワークに制限がある場合も考えられ、その意味でも異なるフレームワークに書き換える経験を積み重ねたい。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sprint25-seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
